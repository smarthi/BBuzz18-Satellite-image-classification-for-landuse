<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Large Scale Landuse Classification of Satellite Imagery</title>

    <meta name="description" content="Building streaming pipelines for neural machine translation">
    <meta name="author" content="Kellen Sunderland; Suneel Marthi">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="font-awesome-4.7.0/css/font-awesome.min.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/bbuzz17.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
    <!--[if lt IE 9]> <script src="lib/js/html5shiv.js"></script> <![endif]-->
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-background-image="img/brandenburgerTor.jpg">
          <br/>
          <br/>
          <br/>
          <h3>Large Scale Landuse Classification of Satellite Imagery</h3>
          <br/>
          <p>Suneel Marthi
          </p>
          <p style="font-size: 60%">June 11, 2018<br/>
             Berlin Buzzwords, Berlin, Germany</p>
        </section>
				<section data-background-image="img/ZsMwlm2.jpg">
          <h3>$WhoAmI</h3>
          <h6 style='text-align: left; font-size: 80%;'>Suneel Marthi <br/><span style="font-size: 60%"><i class="fa fa-twitter" aria-hidden="true"></i> @suneelmarthi</span></h6>
          <ul style='font-size: 70%;'>
            <li>Member of Apache Software Foundation</li>
            <li>Committer and PMC on Apache Mahout, Apache OpenNLP, Apache Streams</li>
          </ul>
        </section>
        <section data-background-image="img/ZsMwlm2.jpg">
          <h3>Agenda</h3>
          <ul>
            <li>Introduction</li>
            <li>Data description</li>
            <li>Cloud classification</li>
            <li>Segmentation</li>
          </ul>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
          <h4>Goal: Identify Tulip fields from Sentinel-2 satellite images</h4>
            <div style="background: #c9c9c9;">
                <img src='img/tulip-fields.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>

        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Workflow</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide3.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h3>Data: Sentinel-2</h3>
            <p style='text-align: left'>Earth observation mission from ESA</p>
            <p style='text-align: left'>13 spectral bands, from RGB to SWIR (Short Wave Infrared)</p>
            <p style='text-align: left'>Spatial resolution: 10m/px (RGB bands)</p>
            <p style='text-align: left'>5 day revisit time</p>
            <p style='text-align: left'>Free and open data policy</p>
            <img src='img/slide5.png' style=' position: absolute; right: 0px; bottom: 0px; width: 30%; height: 30%; margin: 0px, 0px, 0px, 0px; border: none; background: none;box-shadow: none; '/><br/>
        </section>

        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <h3>Data acquisition</h3>
            <p style='text-align: left'>Images downloaded using Sentinel Hub’s WMS (web mapping service)</p>
            <p style='text-align: left'>Download tool from Matthieu Guillaumin (@mguillau)</p>
            <table>
                <td><img src='img/slide6.png' style='margin: 0px, 0px, 0px, 0px; border: none; background: none;box-shadow: none; '/><br/></td>
                <td><img src='img/slide6-2.png' style='margin: 0px, 0px, 0px, 0px; border: none; background: none;box-shadow: none; '/><br/></td>
            </table>
        </section>
        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <h3>Data</h3>
            <p style='text-align: left'>256 x 256 px images, RGB</p>
            <img src='img/slide7.png' style='margin: 0px, 0px, 0px, 0px; border: none; background: none;box-shadow: none; '/><br/>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Workflow</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide3.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Cloud filtering</h4>
            <p style='text-align: left'>Need to remove cloudy images before segmenting</p>
            <p style='text-align: left'>Approach: train a Neural Network to classify images as clear or cloudy</p>
            <p style='text-align: left'>Tested CNNs: ResNet50 and ResNet101</p>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Cloud filtering: training data</h4>
            <p style='text-align: left'>‘Planet: Understanding the Amazon from Space’ Kaggle competition</p>
            <p style='text-align: left'>40K images labeled as clear, hazy, partly cloudy or cloudy</p>
            <img src='img/slide10.png' style='margin: 0px, 0px, 0px, 0px; border: none; background: none;box-shadow: none; '/><br/>
        </section>
        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <p>Cloud Filtering: Training data(2)</p>
            <table>
                <thead>
                <tr>
                    <th>Origin</th><th>No. of Images</th><th>Cloudy Images</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Kaggle Competition</td><td>40000</td><td>30%</td>
                </tr>
                <tr>
                    <td>Sentinel-2(hand labelled)</td><td>5000</td><td>50%</td>
                </tr>
                <tr>
                    <td>Total</td><td>45000</td><td>32%</td>
                </tr>
                </tbody>
            </table>
            <p>Only two classes: clear and cloudy (cloudy = haze + partly cloudy + cloudy) </p>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Training data split</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide12.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>

        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <p>Results</p>
            <table>
                <thead>
                <tr>
                    <th>Model</th><th>Accuracy</th><th>F1</th><th>Epochs (train + finetune)</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>ResNet50</td><td>0.983</td><td>0.986</td><td>23 + 7</td>
                </tr>
                <tr>
                    <td>ResNet101</td><td>0.978</td><td>0.982</td><td>43 + 9</td>
                </tr>
                </tbody>
            </table>
            <p>Choose ResNet50 for filtering cloudy images</p>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Example Results</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide14.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Workflow</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide15.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Segmentation Goals</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide16.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>Approach U-Net</h3>
            <ul>
                <li>State of the Art CNN for Image Segmentation</li>
                <li>Commonly used with biomedical images</code></li>
                <li>Best Architecture for tasks like this</li>
            </ul>
            <p style="font-size: 35%;"><i>O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. arxiv:1505.04597, 2015</i></p>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>U-Net Architecture</h4>
            <div style="background: #c9c9c9;">
                <img src='img/slide18.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>U-Net Building Blocks</h3>
            <pre><code data-trim data-noescape>
                def conv_block(channels, kernel_size):
                   out = nn.HybridSequential()
                   out.add(
                       nn.Conv2D(channels, kernel_size, padding=1, use_bias=False),
                       nn.BatchNorm(),
                       nn.Activation('relu')
                   )
                   return out
            </code></pre>
            <pre><code data-trim data-noescape>
                def down_block(channels):
                   out = nn.HybridSequential()
                   out.add(
                       conv_block(channels, 3),
                       conv_block(channels, 3)
                   )
                   return out
            </code></pre>
        </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>U-Net Building Blocks (2)</h3>
            <pre><code data-trim data-noescape>
                class up_block(nn.HybridBlock):
                   def __init__(self, channels, shrink=True, **kwargs):
                       super(up_block, self).__init__(**kwargs)
                       self.upsampler = nn.Conv2DTranspose(channels=channels, kernel_size=4,
                                                            strides=2, padding=1, use_bias=False)
                       self.conv1 = conv_block(channels, 1)
                       self.conv3_0 = conv_block(channels, 3)
                       if shrink:
                           self.conv3_1 = conv_block(int(channels/2), 3)
                       else:
                           self.conv3_1 = conv_block(channels, 3)
                   def hybrid_forward(self, F, x, s):
                       x = self.upsampler(x)
                       x = self.conv1(x)
                       x = F.relu(x)
                       x = F.Crop(*[x,s], center_crop=True)
                       x = s + x
                       x = self.conv3_0(x)
                       x = self.conv3_1(x)
                       return x
            </code></pre>
        </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>U-Net: Training data</h3>
            <table>
                <tbody>
                    <tr>
                        <td>
                            <ul>
                                <li>Ground truth: tulip fields in the Netherlands</li>
                                <li>Provided by Geopedia, from Sinergise</li>
                                <img src='img/slide21.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
                            </ul>
                        </td>
                        <td>
                            <img src='img/slide21-2.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
                        </td>

                    </tr>
                </tbody>
            </table>
        </section>
                <section data-background-image="img/ZsMwlm2-yellowish.jpg">
                    <h3>Data augmentation
                    </h3>
                    <table>
                        <tbody>
                        <tr>
                            <td>
                              <img src='img/slide-aug.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
                            </td>
                            <td>
                                <pre><code data-trim data-noescape>import Augmentor

p = Augmentor.Pipeline(img_dir)

p.skew(probability=0.5, magnitude=0.5)
p.shear(probability=0.3, max_shear=15)
p.flip_left_right(probability=0.5)
p.flip_top_bottom(probability=0.5)
p.rotate_random_90(probability=0.75)
p.rotate(probability=0.75, max_rotation=20)
 </code></pre>
                            </td>
                        </tr>
                        </tbody>
                    </table>
                </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>Alignment</h3>
            <ul>
                <li>In a parallel text (or when we translate), we align words in one language with the word in the other<br/>
                    <table>
                        <tr>
                            <td>Das</td><td>Geb&auml;ude</td><td>ist</td><td>hoch</td>
                        </tr>
                        <tr>
                            <td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td>
                        </tr>
                        <tr>
                            <td>the</td><td>building</td><td>is</td><td>high</td>
                        </tr>
                    </table>
                </li>
                <li>Word positions are numbered 1&mdash;4</li>
            </ul>
        </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>Alignment Function</h3>
            <ul>
                <li>Define the Alignment with an Alignment Function</li>
                <li>Mapping an English target word at position <code>i</code> to <code>a</code> German source word at position <code>j</code> with a function <code>a : i &rarr; j</code></li>
                <li>Example</li>
            </ul>
            <pre><code data-trim data-noescape>a : {1 &rarr; 1, 2 &rarr; 2, 3 &rarr; 3, 4 &rarr; 4}</code></pre>
        </section>
        <section data-background-image="img/ZsMwlm2-blueish.jpg">
            <h3>One-to-Many Translation</h3>
            <p>A source word could translate into multiple target words</p>
            <table>
                <tr>
                    <td>Das</td><td>ist</td><td>ein</td><td colspan='3' style='text-align: center;'>Hochhaus&nbsp;&nbsp;&nbsp;</td>
                </tr>
                <tr>
                    <td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&#8601;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&#8600;</td>
                </tr>
                <tr>
                    <td>This</td><td>is</td><td>a</td><td>high&nbsp;&nbsp;&nbsp;</td><td>rise</td><td>building</td>
                </tr>
            </table>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
            <h3>Phrase-based Translation</h3>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
            <h3>Phrase-Based Model</h3>
            <table style='font-size: 80%;'>
                <tr>
                    <td>Berlin</td><td>ist</td><td>ein</td><td>herausragendes</td><td>Kunst- und Kulturzentrum</td><td>.</td>
                </tr>
                <tr>
                    <td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td><td style='text-align: center;'>&darr;</td>
                </tr>
                <tr>
                    <td>Berlin</td><td>is</td><td>an</td><td>outstanding</td><td>Art and cultural center</td><td>.</td>
                </tr>
            </table>
            <br/>
            <ul>
                <li>Foreign input is segmented in phrases</li>
                <li>Each phrase is translated into English</li>
                <li>Phrases are reordered</li>
            </ul>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
            <h4>Example Data Augmentation</h4>
            <div style="background: #c9c9c9;">
                <img src='img/DataAugmentation.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            </div>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h4>Loss function: Soft Dice Coefficient loss</h4>
            <img src='img/slide22.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            <p style='...'>Prediction = Probability of each pixel belonging to a Tulip Field (Softmax output)</p>
            <p style='...'>ε serves to prevent division by zero</p>
        </section>
        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <h4>Evaluation Metric: IoU</h4>
            <img src='img/slide23.png' style='margin: 0px, 20px, 0px, 20px; border: none; background: none;box-shadow: none; '/><br/>
            <p style='...'><i>Aka</i> Jaccard Index</p>
            <p style='...'>Similar to Dice coefficient, standard metric for image segmentation </p>
        </section>
        <section data-background-image="img/ZsMwlm2-yellowish.jpg">
            <h3>Results</h3>
            <ul>
                <li><b>IoU = 0.73</b> after 23 training epochs</li>
                <li>Related results: DSTL Kaggle competition</li>
                <li>IoU = 0.84 on crop vs building/road/water/etc segmentation</li>
            </ul>
            <p style="font-size: 35%;"><i>https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/discussion/29790</i></p>
        </section>

        <section data-background-image="img/ZsMwlm2-redish.jpg">
          <h3>TVM</h3>
            <p style='font-size: 75%;'>TVM is a Tensor intermediate representation(IR) stack for deep learning systems. It is designed to close the gap between the productivity-focused deep learning frameworks, and the performance- and efficiency-focused hardware backends. TVM works with deep learning frameworks to provide end to end compilation to different backends. </p>
            <p style='font-size: 50%;'>https://github.com/dmlc/tvm</p>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
          <h3>Alibaba TVM Optimization</h3>
            <img src='img/transformer.png' style='width: 300px; border: none; background: none;box-shadow: none; '/>
            <p style='font-size: 50%;'>http://tvmlang.org/2018/03/23/nmt-transformer-optimize.html</p>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
          <h3>Alibaba TVM Optimization</h3>
            <img src='img/tvm_speedup.png' style='width: 400px; border: none; background: none;box-shadow: none; '/>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
          <h3>Facebook - Tensor Comprehensions</h3><br/>
          <img src='img/tc_evol_slower.gif' style='width: 400px; border: none; background: none;box-shadow: none; '/><br/>
          <p style='font-size: 50%;'>https://research.fb.com/announcing-tensor-comprehensions/</p>
        </section>
        <section data-background-image="img/ZsMwlm2-purpleish.jpg">
            <h3>Streaming Pipelines for Satellite Image Classification</h3>
        </section>
        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <h3>Complete Pipeline (Flink)</h3>
            <img src='img/Complete-pipeline.png' style='max-width: 80%; border: none; background: none;box-shadow: none; '/>
        </section>
        <section data-background-image="img/ZsMwlm2-greenish.jpg">
            <h3>NMT Inference Pipeline</h3>
            <img src='img/pipeline.png' style='max-width: 80%; border: none; background: none;box-shadow: none; '/>
        </section>
        <section data-background-image="img/buzzwords_2017-blueish.png">
          <h3>Credits</h3>
          <ul>
            <li>Jose Contreras, Matthieu Guillaumin, Kellen Sunderland (Amazon - Berlin)</li>
            <li>Ali Abbas (HERE - Frankfurt)</li>
            <li>Apache Beam: Pable Estrada, Lukasz Cwik, Sergei Sokolenko (Google)</li>
            <li>Pascal Hahn, Jed Sundvall (Amazon - Germany)</li>
            <li>Apache OpenNLP: Bruno Kinoshita, Joern Kottmann</li>
          </ul>
        </section>
        <section data-background-image="img/ZsMwlm2.jpg">
          <h2>Links</h2>
          <ul>
            <li>Earth on AWS: https://aws.amazon.com/earth/</li>
            <li>Semantic Segmentation - U-Net: https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066</li>
            <li>ResNet: https://arxiv.org/pdf/1512.03385.pdf</li>
            <li>U-Net: https://arxiv.org/pdf/1505.04597.pdf</li>
            <li>Slides: https://smarthi.github.io/BBuzz18-Satellite-image-classification-for-landuse/</li>
            <li>Code: https://github.com/smarthi/satellite-images</li>
            <li>Apache Beam: https://beam.apache.org</li>
          </ul>
        </section>
        <section data-background-image="img/ZsMwlm2.jpg">
          <h2>Questions ???</h2>
        </section>
        <section data-background-image="img/ZsMwlm2-redish.jpg">
          <h3>Sockeye Model Types</h3>
          <ul>
              <li>RNN Models</li>
              <li>Convolutional Models</li>
              <li>Transformer Models</li>
          </ul>
        </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
        controls: false,
        progress: true,
        history: true,
        center: true,
        slideNumber: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        keyboard: true,
        touch: true,
        loop: false,
        fragments: true,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
